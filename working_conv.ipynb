{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n1ShEIwvBBB",
        "outputId": "481a36a8-811e-44f8-911d-f0d41df19c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/EE559-MiniProject/Proj_341752_337188_250222\n"
          ]
        }
      ],
      "source": [
        "cd Proj_341752_337188_250222"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPrEpCciFNQb",
        "outputId": "895da9c4-3a3c-44da-a7bd-62055823c66f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test2.py -p Proj_341752_337188_250222\\  -d Proj_341752_337188_250222/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bj8_v1SvsMH",
        "outputId": "cad9e8d5-40be-4964-a82a-013645a240d7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m!!! Warning: Project folder name must be in the form Proj_XXXXXX_XXXXXX_XXXXXX\u001b[39m\n",
            "\n",
            "=============\n",
            "> Testing folder structure ...\n",
            "\n",
            "=============\n",
            "> Testing forward dummy input ...\n",
            "\n",
            "=============\n",
            "> Testing blocks ...\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnEYUaKovylw",
        "outputId": "6b8df50a-6fd0-453e-f75f-eeb151933e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/EE559-MiniProject/Proj_341752_337188_250222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[32]:\n",
        "\n",
        "\n",
        "#from module import Module\n",
        "import torch\n",
        "from torch.nn.functional import fold, unfold\n",
        "import math\n",
        "\n",
        "epsilon = 1e-6\n",
        "\n",
        "class Module:\n",
        "\n",
        "\tdef forward(self, *input):\n",
        "\t\traise NotImplementedError\n",
        "\n",
        "\tdef backward(self, *gradwrtoutput):\n",
        "\t\traise NotImplementedError\n",
        "\n",
        "\tdef param(self):\n",
        "\t\treturn []\n",
        "\n",
        "class Conv2d(Module):\n",
        "    def __init__(self, input_shape, out_channels,kernel_size ,bias=True,stride=1, padding=1):\n",
        "        \n",
        "        self.in_channels, self.input_height, self.input_width =input_shape\n",
        "        \n",
        "        self.stride=stride\n",
        "        self.padding=padding\n",
        "        \n",
        "        self.out_channels=out_channels\n",
        "        self.kernel_size=kernel_size\n",
        "        \n",
        "        self.kernel_shape=(out_channels,self.in_channels,kernel_size[0],kernel_size[1])\n",
        "        \n",
        "        output_height=torch.tensor(self.input_height-self.kernel_size[0]+2*self.padding).div(self.stride).add(1).ceil().int()\n",
        "        output_width=torch.tensor(self.input_width-self.kernel_size[1]+2*self.padding).div(self.stride).add(1).ceil().int()\n",
        "        \n",
        "        self.output_shape=(self.out_channels, output_height,output_width )\n",
        "\n",
        "        self.bias=None\n",
        "        \n",
        "        self.x=None\n",
        "        \n",
        "        self.w=torch.empty(self.kernel_shape).normal_(0, epsilon)\n",
        "        \n",
        "        if bias:\n",
        "            self.bias=torch.empty(out_channels).normal_(0, epsilon)\n",
        "            \n",
        "    def forward(self, x):\n",
        "                         \n",
        "        self.x = x\n",
        "        x_unf = unfold(x , kernel_size = self.kernel_size, padding=self.padding)\n",
        "        output_unf = self.w.view(self.out_channels,-1)@x_unf+self.bias.view(1 , -1 , 1)\n",
        "        output= fold(output_unf,(self.output_shape[1],self.output_shape[2]), (1,1) , self.stride )\n",
        "        \n",
        "        return output\n",
        "        \n",
        "                           \n",
        "    def backward(self,gradwrtoutput):\n",
        "        \n",
        "        self.dl_dw=torch.empty(self.kernel_shape).zero_()\n",
        "        self.dl_db=torch.empty(self.out_channels).zero_()\n",
        "        self.dl_dx=torch.empty(input_shape).zero_()\n",
        "\n",
        "        dl_dz_reshaped = gradwrtoutput.permute(1, 2, 3, 0).reshape(self.out_channels, -1) \n",
        "        x_unfolded = unfold(self.x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding) \n",
        "        x_unfolded_reshaped =x_unfolded.permute(2, 0, 1).reshape(dl_dz_reshaped.shape[1], -1)\n",
        "\n",
        "\n",
        "        # Find gradient wrt weights\n",
        "        self.dl_dw = (dl_dz_reshaped @ x_unfolded_reshaped).reshape(self.w.shape)\n",
        "\n",
        "        # Find gradient wrt bias\n",
        "        self.dl_db = gradwrtoutput.sum(axis = (0, 2, 3)) \n",
        "\n",
        "        # Find gradient wrt Input\n",
        "        dl_dx= (self.w.reshape(self.out_channels, -1).t() @ dl_dz_reshaped).reshape(x_unfolded.permute(1, 2, 0).shape).permute(2, 0, 1)\n",
        "\n",
        "        \n",
        "        dim_inp = (self.x.shape[2], self.x.shape[3])\n",
        "        self.dl_dx=fold(dl_dx, dim_inp, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
        "\n",
        "        return  self.dl_dx,self.dl_dw,self.dl_db\n",
        "        \n",
        "        \n",
        "        \n",
        "    \n",
        "    def param(self):\n",
        "        return [(self.w, self.dl_dw), (self.bias, self.dl_db)]\n",
        "        \n",
        "                           \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "# In[33]:\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "s_1, s_2 = 100,100\n",
        "k_1, k_2 = 3,3\n",
        "bs = 2\n",
        "ch_in, ch_out = 2, 4\n",
        "stride = 1\n",
        "\n",
        "input_shape=(ch_in,s_1,s_2)\n",
        "\n",
        "# input tensor \n",
        "X = torch.empty(bs, ch_in, s_1, s_2).normal_().requires_grad_()\n",
        "X_copy = X.clone().detach().requires_grad_()\n",
        "\n",
        "# initialize convolution moduls\n",
        "conv = Conv2d(input_shape,ch_out, kernel_size = (k_1, k_2), bias=True, stride = stride)\n",
        "\n",
        "# get weigts and bias\n",
        "F = conv.w\n",
        "B = conv.bias\n",
        "F.requires_grad_()\n",
        "B.requires_grad_()\n",
        "\n",
        "# forward\n",
        "out = conv.forward(X)\n",
        "out_compare = torch.nn.functional.conv2d(X_copy, F, bias = B, stride = stride, padding=1)\n",
        "\n",
        "# backward\n",
        "dL_dX,dL_dF, dL_dB = conv.backward(out/out)\n",
        "print(dL_dF.shape)\n",
        "out_compare.backward(out_compare/out_compare)\n",
        "\n",
        "res1=(out_compare - out).abs().sum()\n",
        "res1\n",
        "\n",
        "print('same output of conv: ', (out_compare - out).abs().sum()) \n",
        "print('same input gradient: ', (X_copy.grad - dL_dX).abs().sum())\n",
        "print('same weigth gradient: ',(F.grad-dL_dF).abs().sum() )\n",
        "print('same bias gradient: ',(B.grad-dL_dB).abs().sum() )\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nyL2x8bKNrR",
        "outputId": "5bf5d903-2dd3-49f2-8bfe-8375ccf2de85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2, 3, 3])\n",
            "same output of conv:  tensor(2.4021e-08, grad_fn=<SumBackward0>)\n",
            "same input gradient:  tensor(4.3979e-08, grad_fn=<SumBackward0>)\n",
            "same weigth gradient:  tensor(0.0182, grad_fn=<SumBackward0>)\n",
            "same bias gradient:  tensor(0., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mr_5uF_zKU_G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}